{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.functional import elu\n",
    "import torch.nn.functional as F\n",
    "from utils import calculate_same_padding\n",
    "import sys\n",
    "\n",
    "from train_utils import *\n",
    "\n",
    "\n",
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *args, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(self.weight.data, p=2, dim=0,\n",
    "                                        maxnorm=self.max_norm)\n",
    "        return super(Conv2dWithConstraint, self).forward(x)\n",
    "\n",
    "# Implementation of EEGNet.\n",
    "# TODO: Test and train this model.\n",
    "\n",
    "# Possible evaluation parameters\n",
    "# all, emg, state, eeg\n",
    "\n",
    "\n",
    "class McannV2(nn.Module):\n",
    "\n",
    "    def __init__(self, T, F1, D, num_classes=2, dropout=0.25, channels=32,\n",
    "                 evaluation_param=\"all\", add_noise=True,\n",
    "                 conditional_channels=8, is_kaggle=False, is_bci=False, noise=1.0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.evaluation_param = evaluation_param\n",
    "        self.add_noise = add_noise\n",
    "        self.is_kaggle = is_kaggle\n",
    "        self.is_bci = is_bci\n",
    "\n",
    "        self.noise = noise\n",
    "\n",
    "        self.use_gpu = torch.cuda.is_available(\n",
    "        ) and os.environ['USE_CUDA'] == 'True'\n",
    "        self.kernel_length = 10\n",
    "\n",
    "        self.conditional_channels = conditional_channels\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.latent_dims = 64\n",
    "\n",
    "        if is_bci:\n",
    "            self.T = 384\n",
    "            self.channels = 22\n",
    "        elif is_kaggle:\n",
    "            self.T = 160\n",
    "            self.channels = 56\n",
    "        else:\n",
    "            self.T = T\n",
    "\n",
    "        self.F1 = 10\n",
    "        self.D = D\n",
    "        self.F2 = int(F1 * D)\n",
    "        F2 = self.F1 * D\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.dropout_rate = dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Shared filters (EMG, EOG, EEG)\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            1, self.F1, (1, self.kernel_length), stride=1, bias=False,\n",
    "            padding=(0, self.kernel_length // 2))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(\n",
    "            F1, momentum=0.01, affine=True, eps=1e-3\n",
    "        )\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            self.F1, self.F1, (1, self.kernel_length), stride=(1, 1), bias=False, groups=self.F1,\n",
    "            padding=(0, self.kernel_length // 2)\n",
    "        )\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(\n",
    "            self.F1, momentum=0.01, affine=True, eps=1e-3\n",
    "        )\n",
    "\n",
    "        # EEG\n",
    "\n",
    "        self.depth_conv_1 = Conv2dWithConstraint(\n",
    "            self.F1, self.F1 * self.D, (self.channels, 1), max_norm=1, stride=1, bias=False,\n",
    "            groups=self.F1,\n",
    "            padding=(0, 0))\n",
    "\n",
    "        # context\n",
    "        self.cond_conv_2 = nn.Conv2d(\n",
    "            self.F1, self.F1, (1, self.kernel_length), stride=(1, 1), bias=False, groups=self.F1,\n",
    "            padding=(0, self.kernel_length // 2)\n",
    "        )\n",
    "        self.depth_cond_conv_1 = Conv2dWithConstraint(\n",
    "            self.F1, self.F1 * self.D, (self.conditional_channels, 1), max_norm=1, stride=1, bias=False,\n",
    "            groups=self.F1,\n",
    "            padding=(0, 0))\n",
    "\n",
    "        self.batch_norm_cond = nn.BatchNorm2d(\n",
    "            self.F1, momentum=0.01, affine=True, eps=1e-3)\n",
    "\n",
    "        # FUSION\n",
    "        # Method 1: Fuse using Linear layer + 1 for State information\n",
    "        if self.evaluation_param == \"all\":\n",
    "            self.fuse = nn.Linear(self.D * self.F1 * 2 + 1, self.D * self.F1)\n",
    "        elif self.evaluation_param == \"emg\":\n",
    "            self.fuse = nn.Linear(self.D * self.F1 * 2, self.D * self.F1)\n",
    "        elif self.evaluation_param == \"state\":\n",
    "            self.fuse = nn.Linear(self.D * self.F1 + 1, self.D * self.F1)\n",
    "        elif self.evaluation_param == \"eeg\":\n",
    "            self.fuse = nn.Linear(self.D * self.F1, self.D * self.F1)\n",
    "\n",
    "        # Method 2: Add\n",
    "        self.fuse_norm = nn.BatchNorm2d(\n",
    "            self.F1 * self.D, momentum=0.01, affine=True, eps=1e-3)\n",
    "\n",
    "        self.fuse_pool = nn.AvgPool2d(\n",
    "            kernel_size=(1, 4), stride=(1, 4))\n",
    "\n",
    "        # Block 2\n",
    "        padding_h, padding_w = calculate_same_padding(1, T / 4, 1, 16)\n",
    "        # +1 or not depends on even/odd\n",
    "        self.separate_conv_layer_1 = nn.Conv2d(\n",
    "            self.F1 * self.D, self.F1 * self.D, (1, 16), stride=1, bias=False, groups=self.F1 * self.D,\n",
    "            padding=(0, 16 // 2))\n",
    "\n",
    "        self.separate_conv_layer_2 = nn.Conv2d(\n",
    "            self.F1 * self.D, self.F2, (1, 1), stride=1, bias=False,\n",
    "            padding=(0, 0))\n",
    "\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(\n",
    "            self.F2, momentum=0.01, affine=True, eps=1e-3)\n",
    "\n",
    "        self.avg_pool_2 = nn.MaxPool2d(kernel_size=(1, 8), stride=(1, 8))\n",
    "\n",
    "        # merge signals time wise\n",
    "        if self.is_kaggle:\n",
    "            self.depth_merge = nn.Linear(100, self.latent_dims)\n",
    "        elif self.is_bci:\n",
    "            self.depth_merge = nn.Linear(240, self.latent_dims)\n",
    "        elif self.T == 512: # noisy dataset\n",
    "            self.depth_merge = nn.Linear(320, self.latent_dims)\n",
    "        elif self.T == 256:\n",
    "            self.depth_merge = nn.Linear(160, self.latent_dims)\n",
    "        elif self.T == 128:\n",
    "            self.depth_merge = nn.Linear(80, self.latent_dims)\n",
    "        elif self.T == 64:\n",
    "            self.latent_dims = min(40, self.latent_dims)\n",
    "            self.depth_merge = nn.Linear(40, self.latent_dims)\n",
    "        else:\n",
    "            self.depth_merge = nn.Linear(220, self.latent_dims)\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Tensor cross product\n",
    "        # self.classify = nn.Linear(F2 * 1 * (T // 32), self.num_classes)\n",
    "        self.classify = nn.Linear(self.latent_dims, self.num_classes)\n",
    "\n",
    "        # self.depth_merge = nn.Linear(32, self.latent_dims)\n",
    "        # self.classify = nn.Linear(32, self.num_classes)\n",
    "\n",
    "        # Decoder network\n",
    "        self.un_merge = nn.Linear(self.latent_dims, F2 * 1 * (T // 32))\n",
    "\n",
    "        self.unpool2 = nn.Linear(F2 * 1 * (T // 32), F2 * 1 * (T // 32) * 8)\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm2d(F2)\n",
    "        self.separable_deconv2 = nn.ConvTranspose2d(F2, F2, (1, 1))\n",
    "        padding_h, padding_w = calculate_same_padding(1, T / 4, 1, 16)\n",
    "        self.separable_deconv1 = nn.ConvTranspose2d(\n",
    "            F2, F2, (1, 16), padding=(0, padding_w))\n",
    "\n",
    "        if self.is_kaggle:\n",
    "            self.unpool1 = nn.Linear(4 * (T//16), 16 * T//16 + 1)\n",
    "        else:\n",
    "            self.unpool1 = nn.Linear(4 * (T//16) + 1, 16 * T//16 + 1)\n",
    "\n",
    "        self.depth_deconv1 = nn.ConvTranspose2d(\n",
    "            self.F1*D, self.F1, (self.channels, 1), padding=0, groups=self.F1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            self.F1, 1, (1, self.kernel_length // 2), padding=(0, self.kernel_length // 4))\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            1, 1, (1, self.kernel_length), padding=(0, self.kernel_length // 2 - 1))\n",
    "\n",
    "    def ctx_encoder(self, x):\n",
    "        batch_sz = x.size(0)\n",
    "        input_size = x.size()\n",
    "\n",
    "        # ## add noise mean 0 variance 1\n",
    "        # if self.training:\n",
    "        #     x = x + to_float_tensor(torch.randn(x.size()) * 1)\n",
    "\n",
    "        x = x.view(batch_sz, 1, self.conditional_channels, self.T)\n",
    "\n",
    "        x = self.conv_1(x)  # temporal\n",
    "        x = self.batch_norm_2(x)\n",
    "        # x = self.cond_conv_2(x)\n",
    "        x = self.cond_conv_2(x)\n",
    "        x = self.pool(F.elu(x))\n",
    "        # Input: (N, F1, C, T), Output: (N, D * F1, 1, T)\n",
    "        x = self.depth_cond_conv_1(x)  # depth for sensor information\n",
    "\n",
    "        return x\n",
    "\n",
    "    def eeg_encoder(self, x):\n",
    "        batch_sz = x.size(0)\n",
    "        input_size = x.size()\n",
    "\n",
    "        if self.training and self.add_noise:\n",
    "            x = x + to_float_tensor(torch.randn(x.size()) * self.noise)\n",
    "\n",
    "        x = x.view(batch_sz, 1, self.channels, self.T)\n",
    "\n",
    "        x = self.conv_1(x)  # temporal\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool(F.elu(x))\n",
    "        # Input: (N, F1, C, T), Output: (N, D * F1, 1, T)\n",
    "        x = self.depth_conv_1(x)  # depth for sensor information\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, muscle, user_context):\n",
    "        batch_sz = x.size(0)\n",
    "        input_size = x.size()\n",
    "\n",
    "        eeg = self.eeg_encoder(x)\n",
    "\n",
    "        if self.evaluation_param == \"all\":\n",
    "            muscle = self.ctx_encoder(muscle)\n",
    "            user_context = user_context.view(\n",
    "                batch_sz, 1, 1, 1).repeat(1, 1, 1, eeg.size(-1))\n",
    "            x = torch.cat([eeg, muscle, user_context], 1).squeeze(2)\n",
    "        elif self.evaluation_param == \"emg\":\n",
    "            muscle = self.ctx_encoder(muscle)\n",
    "            x = torch.cat([eeg, muscle], 1).squeeze(2)\n",
    "        elif self.evaluation_param == \"state\":\n",
    "            user_context = user_context.view(\n",
    "                batch_sz, 1, 1, 1).repeat(1, 1, 1, eeg.size(-1))\n",
    "            x = torch.cat([eeg, user_context], 1).squeeze(2)\n",
    "        elif self.evaluation_param == \"eeg\":\n",
    "            x = torch.cat([eeg], 1).squeeze(2)\n",
    "\n",
    "        x = F.elu(x)\n",
    "        x = torch.transpose(x, 2, 1)\n",
    "\n",
    "        x = self.fuse(x)\n",
    "        x = torch.transpose(x, 1, 2).unsqueeze(2)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # block 2 of eegnet\n",
    "        x = self.separate_conv_layer_1(x)\n",
    "        x = self.separate_conv_layer_2(x)\n",
    "        # x = self.batch_norm_3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.avg_pool_2(x)\n",
    "        # x = F.dropout(x)\n",
    "        x = self.dropout(x)\n",
    "        latent = F.elu(self.depth_merge(x.view(batch_sz, -1)))\n",
    "\n",
    "        probs = self.classify(latent).view(batch_sz, self.num_classes)\n",
    "\n",
    "        return probs, latent\n",
    "\n",
    "    def decode(self, latent, input_size):\n",
    "        batch_sz = input_size[0]\n",
    "        # x = latent.view(batch_sz, -1, self.latent_dims)\n",
    "\n",
    "        x = F.elu(self.un_merge(latent))\n",
    "        # print(x.size())\n",
    "\n",
    "        x = F.elu(self.unpool2(x))\n",
    "\n",
    "        # print(x.size())\n",
    "        # x = self.batch_norm3(x.view(batch_sz, ))\n",
    "        x = F.elu(self.separable_deconv2(x.view(batch_sz, self.F2, 1, -1)))\n",
    "        # print(x.size())\n",
    "        x = self.separable_deconv1(x)\n",
    "        # print(x.size())\n",
    "        x = F.elu(self.unpool1(x))\n",
    "        # print(x.size())\n",
    "        x = self.depth_deconv1(x)\n",
    "        # print(x.size())\n",
    "        x = F.elu(self.deconv1(x))\n",
    "        reconstruction = self.deconv2(x).view(batch_sz, self.channels, -1)\n",
    "        reconstruction = reconstruction[:, :, :input_size[-1]]\n",
    "        # print(reconstruction.size())\n",
    "\n",
    "        return reconstruction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
